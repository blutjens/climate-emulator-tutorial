

# environment/computational parameters
seed: 42
num_workers: 1

# debugging and logging
verbose: True
log_weight_histograms: False # If true, wandb will log
  # distribution plots of weights and gradients on every
  # evaluation epoch
path_experiment: './runs/fcnn/default' # Path to current experiment
path_wandb: './runs/fcnn/default' # Store wandb logs
path_checkpoints: './runs/fcnn/default/checkpoints' # Store model checkpoints
path_sweep: './runs/fcnn/default/sweep' # If train.py is run with --sweep, the wandb and checkpoints will be stored in subfolders of this directory
path_sweep_cfg: './runs/fcnn/default/config/sweep.yaml' # Path to hyperparameter sweep config yaml. Only used if args.sweep == True
wandb_project_name: 'emcli-fcnn'

# dataset parameters
# data_root: ''
dtype: 'float32' # datatype for in-, output and model weights
amp: False # Use mixed precision

# model task parameters
in_channels: 1 # Number of input channels
out_channels: 1 # Number of output channels

# training hyperparameters
epochs: 50        # Number of epochs
batch_size: 2     # Batch size
# img_size: [32,32] # height, width
val_percent: 0.11 # Ratio of dataset that is used for validation. Range is [0,1.]
learning_rate: 0.00001 # Learning rate (lr)
lr_patience: 5 # Number of steps on validation loss platenau until lr scheduler reduces lr
weight_decay: 0.001
# loss_function: 'l1' # Loss function, e.g., 'l1' or 'mse'
activation: 'gelu' # Activation fn in every unet layer
# num_extra_convs: 0 # Number of convolutional 
  # layers with residual connection that are 
  # added at the beginning and end of the UNet
# bilinear: False # If true, use bilinear upsampling in UNet decoder use Transposed conv upscaling otherwise

# For FCNN hyperparameter
model_type: 'fcnn'
n_layers: 2 # Number of fully connected layers
n_units: 128 # Number of units within each fully connected layer
resNet: True # Use residual layers in fcnn
n_res_blocks: 1 # Number of residual blocks in fcnn