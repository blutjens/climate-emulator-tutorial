{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-instant models: GHGs(t) -> temperature(t):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prediction task:\n",
    "  - $f_{global}: co2_{global-mean}(t) \\in \\mathbb{R} \\rightarrow tas_{global-mean}(t) \\in \\mathbb{R}$\n",
    "  - $f_{local}: tas_{global-mean}(t) \\in \\mathbb{R} \\rightarrow tas_{0:lat, 0:lon}(t) \\in \\mathbb{R}^{(lat, lon)}$\n",
    "- Models:\n",
    "  - Fully-connected Neural Net (FCNN):\n",
    "    - $f_{global}$: [batch_size,1,1,1] —> (dense layers) -> [batch_size,1,1,1]\n",
    "    - $f_{local}$: [batch_size,1,1,1] -> (dense layers) -> [batch_size,1,1,lat*lon] —> Reshape(batch_size,1,lat,lon)\n",
    "  - UNet:\n",
    "    - [batch_size, 1, 1, 96 * 144] —> Reshape(batch_size, 1, 96, 144) -> UNet -> (batch_size, 1, 96, 144) -> tanh?()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Markovian models: GHGs(t), temperature(t-1) -> temperature(t):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prediction task:\n",
    "\n",
    "$[\\text{co2}_\\text{global-mean}(t), \\text{tas}_{0:Lat, 0:Lon}(t)] \\in \\mathbb{R}^{3}\\times \\mathbb{R}^{(Lat,Lon)} \\rightarrow \\text{tas}_{0:Lat, 0:Lon}(t+1) \\in \\mathbb{R}^{(Lat, Lon)}$\n",
    "\n",
    "- Limitations\n",
    "  - Captures atmospheric heating due to co2. Assumes that the state (tas and co2) captures the full carbon cycle. But it doesn't capture longer term effects like ocean heating, ice loss, etc.\n",
    "  - Spikes in CO2 would not be accurately modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data samples:  1078\n",
      "Number of test data samples:  250\n",
      "Input sample:  torch.Size([1, 2, 96, 144]) torch.float32\n",
      "Output sample:  torch.Size([1, 96, 144]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "#@title code: Create train and test dataset\n",
    "\n",
    "from emcli.dataset.autoregressiveDataset import AutoregressiveDataset\n",
    "\n",
    "len_snippet = 1 # Number of time steps that target is ahead of input.\n",
    "# E.g., [t=0]->[t=3] for len_snippet = 3. This is also the number of\n",
    "# autoregressive model forecasting steps until the loss is applied.\n",
    "ar_dataset_train = AutoregressiveDataset(X_train, Y_train,\n",
    "    len_snippet=len_snippet, split='train')\n",
    "ar_dataset_test = AutoregressiveDataset(X_test, Y_test,\n",
    "    len_snippet=len_snippet, split='test')\n",
    "\n",
    "input_sample, output_sample = ar_dataset_train.__getitem__(idx=0)\n",
    "print('Number of training data samples: ',len(ar_dataset_train))\n",
    "print('Number of test data samples: ',len(ar_dataset_test))\n",
    "print('Input sample: ', input_sample.shape, input_sample.dtype)\n",
    "print('Output sample: ', output_sample.shape, output_sample.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "# We use the test as val set due to our limited data size. This seems acceptable\n",
    "# as we will perform limited amount of hyperparameter tuning.\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "loader_args = dict(batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True)\n",
    "ar_train_loader = DataLoader(ar_dataset_train, shuffle=True, **loader_args)\n",
    "ar_val_loader = DataLoader(ar_dataset_test, shuffle=False, drop_last=True, **loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title code: PushforwardUNet\n",
    "from emcli.models.unet.unet_model import PushforwardUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title code: evaluate.py -> evaluate()\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, dataloader, criterion, device, amp):\n",
    "  model.eval()\n",
    "  n_val = len(dataloader.dataset)\n",
    "  num_val_batches = len(dataloader)\n",
    "  total_loss = 0\n",
    "\n",
    "  # iterate over the validation set\n",
    "  # with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n",
    "  with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
    "    with tqdm(total=n_val, desc='validation.', unit='img', leave=False) as pbar2:\n",
    "      for i, (inputs, targets) in enumerate(dataloader):\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs = inputs.to(device=device)# , memory_format=torch.channels_last)\n",
    "        targets = targets.to(device=device)# , memory_format=torch.channels_last)\n",
    "\n",
    "        pred = model(inputs)\n",
    "\n",
    "        total_loss += criterion(pred, targets)\n",
    "\n",
    "        pbar2.update(batch_size)\n",
    "        pbar2.set_postfix(**{'val MSE/img': total_loss.cpu().numpy() / float(i+1)})\n",
    "\n",
    "  model.train()\n",
    "  return total_loss / max(num_val_batches, 1)\n",
    "\n",
    "# val_score = evaluate(model, ar_val_loader, nn.MSELoss(), device, cfg[\"amp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title code: train.py -> train_model()\n",
    "\n",
    "from torch import optim\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        epochs: int = 5,\n",
    "        batch_size: int = 1,\n",
    "        learning_rate: float = 1e-5,\n",
    "        val_percent: float = 0.1,\n",
    "        save_checkpoint: bool = True,\n",
    "        img_scale: float = 0.5,\n",
    "        amp: bool = False,\n",
    "        weight_decay: float = 1e-8,\n",
    "        momentum: float = 0.999,\n",
    "        gradient_clipping: float = 1.0,\n",
    "        no_wandb: bool = False,\n",
    "        parallel: bool = False,\n",
    "        dir_checkpoint: str = '',\n",
    "        cfg: dict = None,\n",
    "  ):\n",
    "  \"\"\"\n",
    "  Train model\n",
    "  Source: https://github.com/milesial/Pytorch-UNet/blob/2f62e6b1c8e98022a6418d31a76f6abd800e5ae7/train.py#L81\n",
    "  \"\"\"\n",
    "  # (Initialize logging)\n",
    "  n_train = len(train_loader.dataset)\n",
    "  n_val = len(val_loader.dataset)\n",
    "  if not no_wandb:\n",
    "    experiment = wandb.init(project='U-Net', resume='allow', anonymous='must')\n",
    "    experiment.config.update(dict(epochs=epochs, batch_size=batch_size,\n",
    "        learning_rate=learning_rate, val_percent=val_percent,\n",
    "        save_checkpoint=save_checkpoint, img_scale=img_scale, amp=amp))\n",
    "\n",
    "  logging.info(f'''Starting training:\n",
    "      Epochs:          {epochs}\n",
    "      Batch size:      {batch_size}\n",
    "      Learning rate:   {learning_rate}\n",
    "      Training size:   {n_train}\n",
    "      Validation size: {n_val}\n",
    "      Checkpoints:     {save_checkpoint}\n",
    "      Device:          {device.type}\n",
    "      Images scaling:  {img_scale}\n",
    "      Mixed Precision: {amp}\n",
    "  ''')\n",
    "\n",
    "  # Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "  optimizer = optim.RMSprop(model.parameters(),\n",
    "                lr=learning_rate, weight_decay=weight_decay,\n",
    "                momentum=momentum, foreach=True)\n",
    "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min', patience=5)  # goal: minimize MSE\n",
    "  grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "  criterion = nn.MSELoss()\n",
    "  global_step = 0\n",
    "\n",
    "  print('batch_size')\n",
    "  # Begin training\n",
    "  for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n",
    "      for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device=device)# , memory_format=torch.channels_last)\n",
    "        targets = targets.to(device=device)# , memory_format=torch.channels_last)\n",
    "\n",
    "        assert inputs.shape[2] == model.n_channels, \\\n",
    "          f'Network has been defined with {model.n_channels} input channels, ' \\\n",
    "          f'but loaded images have {inputs.shape[2]} channels. Please check that ' \\\n",
    "          'the images are loaded correctly.'\n",
    "\n",
    "        with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
    "          pred = model(inputs)\n",
    "          # todo: check if loss calculates batch correctly\n",
    "          loss = criterion(pred, targets)\n",
    "\n",
    "        # todo: double check if gradients only go back to last network\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        grad_scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "        grad_scaler.step(optimizer)\n",
    "        grad_scaler.update()\n",
    "\n",
    "        pbar.update(inputs.shape[0])\n",
    "        global_step += 1\n",
    "        epoch_loss += loss.item()\n",
    "        if not no_wandb:\n",
    "          experiment.log({\n",
    "              'train loss': loss.item(),\n",
    "              'step': global_step,\n",
    "              'epoch': epoch\n",
    "          })\n",
    "        pbar.set_postfix(**{'avg MSE/img': epoch_loss / float(i+1)})\n",
    "\n",
    "        # Evaluation round\n",
    "        division_step = (n_train // (1 * batch_size))\n",
    "        if division_step > 0:\n",
    "          if global_step % division_step == 0:\n",
    "            histograms = {}\n",
    "            for tag, value in model.named_parameters():\n",
    "              if not no_wandb:\n",
    "                tag = tag.replace('/', '.')\n",
    "                if not (torch.isinf(value) | torch.isnan(value)).any():\n",
    "                  histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n",
    "                if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():\n",
    "                  histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n",
    "\n",
    "            val_score = evaluate(model, val_loader, criterion, device, amp)\n",
    "            scheduler.step(val_score)\n",
    "\n",
    "            logging.info('Validation Dice score: {}'.format(val_score))\n",
    "            try:\n",
    "              experiment.log({\n",
    "                'learning rate': optimizer.param_groups[0]['lr'],\n",
    "                'validation Dice': val_score,\n",
    "                'inputs': wandb.Image(inputs[0].cpu()),\n",
    "                'predictions': {\n",
    "                  'true': wandb.Image(targets[0].float().cpu()),\n",
    "                  'pred': wandb.Image(pred.argmax(dim=1)[0].float().cpu()),\n",
    "                },\n",
    "                'step': global_step,\n",
    "                'epoch': epoch,\n",
    "                **histograms\n",
    "              })\n",
    "            except:\n",
    "              pass\n",
    "\n",
    "    if save_checkpoint:\n",
    "      Path(dir_checkpoint).mkdir(parents=True, exist_ok=True)\n",
    "      state_dict = model.state_dict()\n",
    "      torch.save(state_dict, str(Path(dir_checkpoint) / 'checkpoint_epoch{}.pth'.format(epoch)))\n",
    "      logging.info(f'Checkpoint {epoch} saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32m/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb Cell 49\u001b[0m line \u001b[0;36m2\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_model(\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m   model\u001b[39m=\u001b[39mmodel,\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m   train_loader\u001b[39m=\u001b[39mar_train_loader,\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m   val_loader\u001b[39m=\u001b[39mar_val_loader,\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m   epochs\u001b[39m=\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m],\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m   batch_size\u001b[39m=\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m],\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m   learning_rate\u001b[39m=\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m],\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m   device\u001b[39m=\u001b[39mdevice,\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m   img_scale\u001b[39m=\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mscale\u001b[39m\u001b[39m\"\u001b[39m],\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m   val_percent\u001b[39m=\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m/\u001b[39m \u001b[39m100.\u001b[39m,\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m   amp\u001b[39m=\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mamp\u001b[39m\u001b[39m\"\u001b[39m],\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m   no_wandb\u001b[39m=\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mno_wandb\u001b[39m\u001b[39m\"\u001b[39m],\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m   dir_checkpoint\u001b[39m=\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mdir_checkpoint\u001b[39m\u001b[39m\"\u001b[39m],\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m   cfg\u001b[39m=\u001b[39mcfg,\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/mnt/c/Users/Bjoern/code/climate-emulator-tutorial/climate_emulator_tutorial.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m )\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "  model=model,\n",
    "  train_loader=ar_train_loader,\n",
    "  val_loader=ar_val_loader,\n",
    "  epochs=cfg[\"epochs\"],\n",
    "  batch_size=cfg[\"batch_size\"],\n",
    "  learning_rate=cfg[\"learning_rate\"],\n",
    "  device=device,\n",
    "  img_scale=cfg[\"scale\"],\n",
    "  val_percent=cfg[\"validation\"] / 100.,\n",
    "  amp=cfg[\"amp\"],\n",
    "  no_wandb=cfg[\"no_wandb\"],\n",
    "  dir_checkpoint=cfg[\"dir_checkpoint\"],\n",
    "  cfg=cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
